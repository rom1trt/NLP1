{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK TOKENIZER AND TF-IDF VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jean-\n",
      "[nltk_data]     michel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from tokenizer import tokenizer\n",
    "from vectorizer import vectorizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../_data/Reviews.csv') # Loading the dataset\n",
    "X, y = data['Text'], data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X[:], y[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  0\n",
      "Number of sentences:  0\n"
     ]
    }
   ],
   "source": [
    "tokenized_documents = tokenizer(X.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X, vect = vectorizer(tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 300) (40000, 300) (160000,) (40000,)\n",
      "[0.4905342  0.53652514 0.60810707 0.53445703 0.47397945 0.63044243\n",
      " 0.77788866 0.57545352 0.62664844 0.49715616 0.59787111 0.406304\n",
      " 0.55857397 0.35807929 0.62644771 0.63388435 0.63886314 0.39206989\n",
      " 0.6117431  0.46993149 0.55822945 0.54868934 0.41539238 0.56931953\n",
      " 0.66263493 0.40275917 0.60358226 0.30975076 0.40173751 0.57448015\n",
      " 0.49435486 0.48770446 0.5582401  0.54829421 0.40585013 0.21431948\n",
      " 0.70002451 0.50918965 0.53844196 0.53878083 0.6792521  0.6003442\n",
      " 0.63211479 0.41174714 0.31062602 0.59575013 0.4498044  0.63131857\n",
      " 0.66233253 0.41354765 0.60187123 0.37521605 0.53285976 0.32667534\n",
      " 0.43353285 0.38850996 0.58945693 0.49794166 0.65526871 0.44370182\n",
      " 0.48311241 0.58029061 0.72859698 0.44735472 0.5844671  0.63727247\n",
      " 0.32446904 0.60332818 0.51567436 0.52645433 0.28422345 0.57145645\n",
      " 0.5578231  0.53274498 0.53082822 0.54409617 0.54493374 0.56921478\n",
      " 0.70193791 0.53333375 0.61360615 0.4092376  0.62622802 0.67085593\n",
      " 0.45017457 0.40164599 0.42532849 0.59416071 0.54216651 0.55083709\n",
      " 0.63001081 0.53831149 0.43242083 0.42716966 0.4394349  0.34061762\n",
      " 0.55356452 0.5823773  0.30259141 0.55183023 0.35662048 0.32233296\n",
      " 0.38190072 0.69718751 0.50251358 0.41267596 0.57699678 0.27740047\n",
      " 0.49798885 0.41245061 0.35438844 0.56982368 0.34424991 0.5523303\n",
      " 0.62237184 0.3784281  0.59790531 0.5707231  0.54820339 0.46880458\n",
      " 0.58954226 0.45992484 0.44924479 0.56308132 0.50510212 0.35016134\n",
      " 0.63979237 0.44424872 0.23864048 0.38942149 0.39471355 0.48180431\n",
      " 0.43410253 0.55167612 0.55191346 0.30476255 0.45561531 0.6648285\n",
      " 0.51256997 0.34768531 0.47414437 0.2717143  0.46603755 0.39481597\n",
      " 0.57607436 0.58486381 0.51109163 0.35570017 0.33485041 0.71536328\n",
      " 0.34538762 0.70549368 0.43284109 0.39235474 0.58405488 0.61170609\n",
      " 0.43624921 0.34767204 0.4449973  0.65606581 0.61345549 0.45061296\n",
      " 0.4952752  0.58147507 0.32534653 0.66264777 0.34606679 0.71843907\n",
      " 0.70688745 0.4510266  0.5185187  0.48818514 0.5903784  0.49905753\n",
      " 0.45570259 0.43967173 0.57894623 0.4311287  0.33988512 0.65275431\n",
      " 0.58983775 0.3908798  0.35510437 0.56040359 0.56112442 0.53300318\n",
      " 0.495492   0.55989571 0.64381948 0.34255501 0.61136069 0.55313037\n",
      " 0.72181831 0.45605149 0.44185114 0.47777898 0.38543232 0.52041491\n",
      " 0.48842736 0.31027914 0.56404532 0.42140377 0.47205214 0.5257858\n",
      " 0.65633111 0.56653698 0.33257411 0.46107459 0.33323884 0.34472096\n",
      " 0.47224069 0.56416299 0.44584731 0.45211523 0.48641976 0.40469716\n",
      " 0.37874726 0.50987031 0.30545812 0.45961019 0.57626458 0.52116025\n",
      " 0.51978433 0.61598232 0.58168517 0.59267317 0.59300041 0.57002859\n",
      " 0.49025989 0.54240343 0.54909627 0.56888386 0.53362834 0.41995805\n",
      " 0.48017573 0.59468706 0.5151111  0.4793952  0.66667452 0.59412521\n",
      " 0.61453866 0.50326967 0.47989314 0.28080022 0.46690564 0.6571583\n",
      " 0.56850802 0.43124304 0.54749302 0.56134969 0.46194949 0.5321081\n",
      " 0.57854978 0.38006277 0.51018887 0.68092639 0.57388964 0.33260618\n",
      " 0.48838145 0.48713901 0.43795803 0.49859346 0.39879006 0.49730378\n",
      " 0.45251054 0.56806189 0.61617464 0.50074846 0.37536214 0.50577301\n",
      " 0.60474189 0.34326138 0.61881285 0.51457617 0.30095289 0.44131478\n",
      " 0.44702737 0.55226214 0.8010085  0.40357201 0.44372272 0.34525018\n",
      " 0.49141335 0.57771544 0.56925052 0.5667924  0.31670509 0.40594544\n",
      " 0.49075877 0.51869807 0.30636828 0.55442961 0.33976787 0.58940148\n",
      " 0.54011178 0.78242876 0.80236808 0.5053866  0.49496604 0.37034806]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network\n",
    "### Model starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Add\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint('model_best.keras', save_best_only=True, monitor='val_loss', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, min_lr=0.00001)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, num_classes):\n",
    "    num_samples = X.shape[0]\n",
    "    while True:\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = to_categorical(y[start:end], num_classes=num_classes)\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "# Setup the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(256)(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(128)(x)\n",
    "x = LeakyReLU()(x)\n",
    "residual = Dense(64)(x)\n",
    "x = LeakyReLU()(residual)\n",
    "\n",
    "x = Dense(64)(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Add()([x, residual])\n",
    "x = Dense(32)(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dense(16)(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "outputs = Dense(np.max(y_train) + 1, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "# Define the generator\n",
    "train_generator = batch_generator(X_train, y_train, 32, np.max(y_train) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.6360 - loss: 1.0801 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m 144/5000\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6516 - loss: 0.9790"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jean-michel/Epita/nlp/.venv/lib/python3.10/site-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n",
      "/home/jean-michel/Epita/nlp/.venv/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_loss available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n",
      "/home/jean-michel/Epita/nlp/.venv/lib/python3.10/site-packages/keras/src/callbacks/callback_list.py:96: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6543 - loss: 0.9604 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6573 - loss: 0.9439 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6587 - loss: 0.9358 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6609 - loss: 0.9280 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6627 - loss: 0.9215 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6627 - loss: 0.9177 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6634 - loss: 0.9147 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6640 - loss: 0.9117 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6655 - loss: 0.9085 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6661 - loss: 0.9071 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.6671 - loss: 0.9044 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6670 - loss: 0.9028 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6682 - loss: 0.9000 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6686 - loss: 0.8983 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6690 - loss: 0.8976 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6687 - loss: 0.8963 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6694 - loss: 0.8952 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6698 - loss: 0.8942 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6693 - loss: 0.8932 - learning_rate: 0.0010\n",
      "Epoch 21/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6702 - loss: 0.8921 - learning_rate: 0.0010\n",
      "Epoch 22/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6713 - loss: 0.8903 - learning_rate: 0.0010\n",
      "Epoch 23/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6726 - loss: 0.8897 - learning_rate: 0.0010\n",
      "Epoch 24/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6715 - loss: 0.8882 - learning_rate: 0.0010\n",
      "Epoch 25/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6719 - loss: 0.8877 - learning_rate: 0.0010\n",
      "Epoch 26/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6725 - loss: 0.8868 - learning_rate: 0.0010\n",
      "Epoch 27/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6721 - loss: 0.8868 - learning_rate: 0.0010\n",
      "Epoch 28/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6723 - loss: 0.8859 - learning_rate: 0.0010\n",
      "Epoch 29/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6724 - loss: 0.8853 - learning_rate: 0.0010\n",
      "Epoch 30/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6728 - loss: 0.8834 - learning_rate: 0.0010\n",
      "Epoch 31/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6739 - loss: 0.8840 - learning_rate: 0.0010\n",
      "Epoch 32/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6733 - loss: 0.8824 - learning_rate: 0.0010\n",
      "Epoch 33/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6736 - loss: 0.8824 - learning_rate: 0.0010\n",
      "Epoch 34/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6748 - loss: 0.8820 - learning_rate: 0.0010\n",
      "Epoch 35/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6740 - loss: 0.8811 - learning_rate: 0.0010\n",
      "Epoch 36/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6748 - loss: 0.8803 - learning_rate: 0.0010\n",
      "Epoch 37/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6747 - loss: 0.8801 - learning_rate: 0.0010\n",
      "Epoch 38/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6746 - loss: 0.8805 - learning_rate: 0.0010\n",
      "Epoch 39/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6750 - loss: 0.8782 - learning_rate: 0.0010\n",
      "Epoch 40/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6753 - loss: 0.8778 - learning_rate: 0.0010\n",
      "Epoch 41/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6754 - loss: 0.8772 - learning_rate: 0.0010\n",
      "Epoch 42/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6759 - loss: 0.8760 - learning_rate: 0.0010\n",
      "Epoch 43/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6760 - loss: 0.8763 - learning_rate: 0.0010\n",
      "Epoch 44/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6758 - loss: 0.8750 - learning_rate: 0.0010\n",
      "Epoch 45/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6766 - loss: 0.8748 - learning_rate: 0.0010\n",
      "Epoch 46/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6763 - loss: 0.8742 - learning_rate: 0.0010\n",
      "Epoch 47/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6770 - loss: 0.8736 - learning_rate: 0.0010\n",
      "Epoch 48/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6768 - loss: 0.8736 - learning_rate: 0.0010\n",
      "Epoch 49/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6766 - loss: 0.8727 - learning_rate: 0.0010\n",
      "Epoch 50/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6769 - loss: 0.8722 - learning_rate: 0.0010\n",
      "Epoch 51/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6776 - loss: 0.8710 - learning_rate: 0.0010\n",
      "Epoch 52/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6775 - loss: 0.8707 - learning_rate: 0.0010\n",
      "Epoch 53/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6782 - loss: 0.8707 - learning_rate: 0.0010\n",
      "Epoch 54/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6779 - loss: 0.8697 - learning_rate: 0.0010\n",
      "Epoch 55/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6777 - loss: 0.8702 - learning_rate: 0.0010\n",
      "Epoch 56/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6784 - loss: 0.8682 - learning_rate: 0.0010\n",
      "Epoch 57/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6788 - loss: 0.8679 - learning_rate: 0.0010\n",
      "Epoch 58/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6785 - loss: 0.8675 - learning_rate: 0.0010\n",
      "Epoch 59/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6795 - loss: 0.8661 - learning_rate: 0.0010\n",
      "Epoch 60/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6788 - loss: 0.8664 - learning_rate: 0.0010\n",
      "Epoch 61/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6792 - loss: 0.8659 - learning_rate: 0.0010\n",
      "Epoch 62/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6796 - loss: 0.8650 - learning_rate: 0.0010\n",
      "Epoch 63/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6798 - loss: 0.8649 - learning_rate: 0.0010\n",
      "Epoch 64/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6796 - loss: 0.8632 - learning_rate: 0.0010\n",
      "Epoch 65/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6800 - loss: 0.8636 - learning_rate: 0.0010\n",
      "Epoch 66/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6802 - loss: 0.8633 - learning_rate: 0.0010\n",
      "Epoch 67/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6810 - loss: 0.8620 - learning_rate: 0.0010\n",
      "Epoch 68/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6801 - loss: 0.8620 - learning_rate: 0.0010\n",
      "Epoch 69/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6806 - loss: 0.8612 - learning_rate: 0.0010\n",
      "Epoch 70/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 944us/step - accuracy: 0.6800 - loss: 0.8610 - learning_rate: 0.0010\n",
      "Epoch 71/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 979us/step - accuracy: 0.6813 - loss: 0.8602 - learning_rate: 0.0010\n",
      "Epoch 72/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6809 - loss: 0.8599 - learning_rate: 0.0010\n",
      "Epoch 73/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6812 - loss: 0.8594 - learning_rate: 0.0010\n",
      "Epoch 74/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 933us/step - accuracy: 0.6810 - loss: 0.8588 - learning_rate: 0.0010\n",
      "Epoch 75/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 919us/step - accuracy: 0.6818 - loss: 0.8583 - learning_rate: 0.0010\n",
      "Epoch 76/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 925us/step - accuracy: 0.6815 - loss: 0.8579 - learning_rate: 0.0010\n",
      "Epoch 77/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6816 - loss: 0.8574 - learning_rate: 0.0010\n",
      "Epoch 78/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6821 - loss: 0.8568 - learning_rate: 0.0010\n",
      "Epoch 79/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6825 - loss: 0.8559 - learning_rate: 0.0010\n",
      "Epoch 80/80\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6823 - loss: 0.8558 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x748d682367a0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=int(np.ceil(X_train.shape[0] / 32)),\n",
    "    epochs=80,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = batch_generator(X_test, y_test, batch_size=32, num_classes=np.max(y_train) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723us/step\n",
      "Confusion Matrix:\n",
      "[[ 2189    27    63    13  1368]\n",
      " [  630    72   160    47  1136]\n",
      " [  476    51   293   196  2152]\n",
      " [  255    21   136   308  5118]\n",
      " [  492    13    67   140 24577]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.60      0.57      3660\n",
      "           2       0.39      0.04      0.06      2045\n",
      "           3       0.41      0.09      0.15      3168\n",
      "           4       0.44      0.05      0.09      5838\n",
      "           5       0.72      0.97      0.82     25289\n",
      "\n",
      "    accuracy                           0.69     40000\n",
      "   macro avg       0.50      0.35      0.34     40000\n",
      "weighted avg       0.62      0.69      0.60     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "predictions_prob = model.predict(test_generator, steps=int(np.ceil(X_test.shape[0] / 32)))\n",
    "predictions = np.argmax(predictions_prob, axis=1)\n",
    "\n",
    "# Metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model with current date and time in model folder\n",
    "\n",
    "# Create a folder named _models in the current directory\n",
    "if not os.path.exists('_models'):\n",
    "    os.makedirs('_models')\n",
    "    \n",
    "model.save(f'_models/config3_feedforward_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m test_sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is a good product\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is a bad product\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThat was bad\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m ]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test_sentences:\n\u001b[0;32m---> 12\u001b[0m     test_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m([sentence])\n\u001b[1;32m     13\u001b[0m     test_vec, _ \u001b[38;5;241m=\u001b[39m vectorizer(test_tokenized)\n\u001b[1;32m     14\u001b[0m     test_vec_lstm \u001b[38;5;241m=\u001b[39m test_vec\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_sentences = [\n",
    "    'This is a good product',\n",
    "    'This is a bad product',\n",
    "    'This is a product',\n",
    "    'This is a very good product',\n",
    "    'This is a very bad product',\n",
    "    'That was bad'\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    test_tokenized = tokenizer([sentence])\n",
    "    test_vec = vect.transform(test_tokenized).toarray()\n",
    "    test_vec_lstm = test_vec.reshape(1, -1)\n",
    "    result = model.predict(test_vec_lstm)\n",
    "    predicted_class = result.argmax()\n",
    "    predicted_score = predicted_class\n",
    "    print(f\"Test sentence: {sentence}\")\n",
    "    print(f\"Predicted score: {predicted_score}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
