{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK TOKENIZER AND TF-IDF VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/assil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from tokenizer import tokenizer\n",
    "from vectorizer import vectorizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../_data/Reviews.csv') # Loading the dataset\n",
    "X, y = data['Text'], data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING : only select first 20000 samples\n",
    "# X, y = X[:20000], y[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_documents = tokenizer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, vect = vectorizer(tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data of length 568454 in batches of size 10000\n",
      "Processing batch from index 0 to 10000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 10000 to 20000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 20000 to 30000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 30000 to 40000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 40000 to 50000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 50000 to 60000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 60000 to 70000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 70000 to 80000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 80000 to 90000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 90000 to 100000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 100000 to 110000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 110000 to 120000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 120000 to 130000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 130000 to 140000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 140000 to 150000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 150000 to 160000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 160000 to 170000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 170000 to 180000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 180000 to 190000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 190000 to 200000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 200000 to 210000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 210000 to 220000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 220000 to 230000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 230000 to 240000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 240000 to 250000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 250000 to 260000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 260000 to 270000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 270000 to 280000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 280000 to 290000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 290000 to 300000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 300000 to 310000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 310000 to 320000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 320000 to 330000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 330000 to 340000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 340000 to 350000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 350000 to 360000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 360000 to 370000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 370000 to 380000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 380000 to 390000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 390000 to 400000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 400000 to 410000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 410000 to 420000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 420000 to 430000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 430000 to 440000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 440000 to 450000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 450000 to 460000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 460000 to 470000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 470000 to 480000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 480000 to 490000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 490000 to 500000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 500000 to 510000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 510000 to 520000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 520000 to 530000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 530000 to 540000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 540000 to 550000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 550000 to 560000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n",
      "Processing batch from index 560000 to 570000\n",
      "Number of tokens:  0\n",
      "Number of sentences:  0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "\n",
    "print(f\"Processing data of length {len(X)} in batches of size {batch_size}\")\n",
    "X_vect_batches = []\n",
    "for start in range(0, len(X), batch_size):\n",
    "    end = start + batch_size\n",
    "    print(f\"Processing batch from index {start} to {end}\")\n",
    "    batch_X = X[start:end].copy()\n",
    "    tokenized_documents = tokenizer(batch_X)\n",
    "    X_vect, _ = vectorizer(tokenized_documents)\n",
    "    # Store the processed batch\n",
    "    X_vect_batches.append(X_vect)\n",
    "\n",
    "X = np.vstack(X_vect_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # most frequent words\n",
    "# print(\"Top 10 most frequent words in the dataset\")\n",
    "# print(vect.get_feature_names_out()[:10])\n",
    "\n",
    "# # least frequent words\n",
    "# print(\"Top 10 least frequent words in the dataset\")\n",
    "# print(vect.get_feature_names_out()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454763, 300) (113691, 300) (454763,) (113691,)\n",
      "[0.69675133 0.46625919 0.19659357 0.54161421 0.49475396 0.68864423\n",
      " 0.77237797 0.61296189 0.64803631 0.67720143 0.53871329 0.43821322\n",
      " 0.57999926 0.60784278 0.64792152 0.49237756 0.62410983 0.48551687\n",
      " 0.66650609 0.6353156  0.28711372 0.57419198 0.26327911 0.58957649\n",
      " 0.58278467 0.46489851 0.33019595 0.34122713 0.50042139 0.47023063\n",
      " 0.23547583 0.40088003 0.36770556 0.21215492 0.44290871 0.29329761\n",
      " 0.39150614 0.41075528 0.67269826 0.2800273  0.59147843 0.74466023\n",
      " 0.64935439 0.45447142 0.53504712 0.59418841 0.35070694 0.20508571\n",
      " 0.42417751 0.48023969 0.40409482 0.63481863 0.44575853 0.45432337\n",
      " 0.44045728 0.50378316 0.26800294 0.76815876 0.40490527 0.42683939\n",
      " 0.6215097  0.43453887 0.51992281 0.53506951 0.58080606 0.52360652\n",
      " 0.54309204 0.47171951 0.43835253 0.65036521 0.4512514  0.70861387\n",
      " 0.63950446 0.58614768 0.42354527 0.74754527 0.66673732 0.70711648\n",
      " 0.60580582 0.34354116 0.45962957 0.76664349 0.34378702 0.61320123\n",
      " 0.4956486  0.39240117 0.70849181 0.48831415 0.39725638 0.60069362\n",
      " 0.67477945 0.75411397 0.69598337 0.45761375 0.50489262 0.7292554\n",
      " 0.36714932 0.48582472 0.62464008 0.61665476 0.32402418 0.23279358\n",
      " 0.16732475 0.5365093  0.34310048 0.60289579 0.35704663 0.28157099\n",
      " 0.56770297 0.51289975 0.30671711 0.54974155 0.18412016 0.35819873\n",
      " 0.5808177  0.37224233 0.46805587 0.56467199 0.38226579 0.56365775\n",
      " 0.60011138 0.73845589 0.45867492 0.30889995 0.31734329 0.2847545\n",
      " 0.7339889  0.37283075 0.57143923 0.59149958 0.69223533 0.40878394\n",
      " 0.4983859  0.5302026  0.69035307 0.21365922 0.21711166 0.45571632\n",
      " 0.43844874 0.28055356 0.64200365 0.36850874 0.41711452 0.62479364\n",
      " 0.71810632 0.45515862 0.57556119 0.44909957 0.41415817 0.44410013\n",
      " 0.40916465 0.42245089 0.67242001 0.27251318 0.58346022 0.49519269\n",
      " 0.49178464 0.41324719 0.6258591  0.27456207 0.64214537 0.34719716\n",
      " 0.41175559 0.37391764 0.65926323 0.83424034 0.46721616 0.47078838\n",
      " 0.72602999 0.37259429 0.63273893 0.62426877 0.71451839 0.3671752\n",
      " 0.37090362 0.48378003 0.48637465 0.42907834 0.57038422 0.63363001\n",
      " 0.65945927 0.29003696 0.28288615 0.31767723 0.66744275 0.66348146\n",
      " 0.39988797 0.43828422 0.36569693 0.53560439 0.58137645 0.38924596\n",
      " 0.57481625 0.75137877 0.77037739 0.46640775 0.42089594 0.52999974\n",
      " 0.37799634 0.60361853 0.44769482 0.65228554 0.74140473 0.46840697\n",
      " 0.57332229 0.82131501 0.46951631 0.50267088 0.14999    0.36981776\n",
      " 0.4817595  0.4373068  0.41752867 0.67327147 0.44226942 0.22245043\n",
      " 0.45032919 0.64339552 0.35232246 0.21938891 0.59381764 0.23487222\n",
      " 0.27487302 0.34235352 0.32922085 0.41090081 0.63312101 0.58452558\n",
      " 0.49136328 0.3999038  0.51946792 0.69070529 0.42191974 0.48360606\n",
      " 0.34664325 0.26453668 0.56894734 0.4482145  0.64092792 0.81541722\n",
      " 0.3849636  0.31058374 0.36895055 0.50387155 0.40263028 0.63799014\n",
      " 0.52124184 0.39624635 0.64089013 0.53213454 0.41525428 0.76692575\n",
      " 0.72011738 0.69701651 0.27638948 0.3298233  0.45862209 0.55994619\n",
      " 0.77894225 0.5117203  0.20578829 0.47886641 0.51921669 0.56912703\n",
      " 0.23745417 0.53454029 0.51384983 0.41615938 0.46988423 0.48096303\n",
      " 0.72748395 0.34939417 0.31502619 0.54111835 0.29923599 0.60124515\n",
      " 0.40881046 0.68766185 0.7966241  0.4679522  0.6455757  0.48485069\n",
      " 0.61512802 0.4408967  0.36467186 0.76844136 0.34028241 0.61460262\n",
      " 0.48084655 0.53648147 0.55653158 0.41603694 0.54797327 0.54597831\n",
      " 0.5687658  0.72684142 0.530299   0.41222903 0.51167561 0.70589562]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "### Model starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 20:56:20.608608: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-18 20:56:22.419326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Add\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint('model_best.keras', save_best_only=True, monitor='val_loss', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, min_lr=0.00001)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 20:56:23.909142: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-18 20:56:24.412614: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-18 20:56:24.412875: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-18 20:56:24.413734: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-18 20:56:24.413919: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-18 20:56:24.414060: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-18 20:56:24.506036: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-18 20:56:24.506305: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-18 20:56:24.506476: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-18 20:56:24.507861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10514 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "/home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 20:56:29.656199: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7105/7105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 51ms/step - accuracy: 0.5174 - loss: 1.4524\n",
      "Epoch 2/5\n",
      "\u001b[1m   3/7105\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:39\u001b[0m 56ms/step - accuracy: 0.6802 - loss: 1.0676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/keras/src/callbacks/early_stopping.py:156: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7105/7105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 52ms/step - accuracy: 0.6380 - loss: 1.1123\n",
      "Epoch 3/5\n",
      "\u001b[1m7105/7105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 52ms/step - accuracy: 0.6380 - loss: 1.0995\n",
      "Epoch 4/5\n",
      "\u001b[1m7105/7105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 52ms/step - accuracy: 0.6380 - loss: 1.0840\n",
      "Epoch 5/5\n",
      "\u001b[1m7105/7105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 53ms/step - accuracy: 0.6421 - loss: 1.0422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x79226e8c37f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def batch_generator(X, y, batch_size, num_classes):\n",
    "    num_samples = X.shape[0]\n",
    "    while True:\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = to_categorical(y[start:end], num_classes=num_classes)\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "# Setup the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation='tanh'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(np.max(y_train) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])\n",
    "\n",
    "# Define the generator\n",
    "train_generator = batch_generator(X_train, y_train, 64, np.max(y_train) + 1)\n",
    "\n",
    "# Train the model with gradient clipping\n",
    "model.fit(train_generator, steps_per_epoch=len(X_train) // 64, epochs=5, \n",
    "          callbacks=[#tf.keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True),\n",
    "                     tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3553/3553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 15ms/step\n",
      "Confusion Matrix:\n",
      "[[10276     0     0     0    50]\n",
      " [ 5816     0     0     0    39]\n",
      " [ 8372     0     0     0   113]\n",
      " [15333     0     0     0   790]\n",
      " [65210     0     0     0  7692]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.10      1.00      0.18     10326\n",
      "           2       0.00      0.00      0.00      5855\n",
      "           3       0.00      0.00      0.00      8485\n",
      "           4       0.00      0.00      0.00     16123\n",
      "           5       0.89      0.11      0.19     72902\n",
      "\n",
      "    accuracy                           0.16    113691\n",
      "   macro avg       0.20      0.22      0.07    113691\n",
      "weighted avg       0.58      0.16      0.14    113691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/assil/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_generator = batch_generator(X_test, y_test, batch_size=32, num_classes=np.max(y_train) + 1)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_prob = model.predict(test_generator, steps=int(np.ceil(X_test.shape[0] / 32)))\n",
    "predictions = np.argmax(predictions_prob, axis=1)\n",
    "\n",
    "# Metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model with current date and time in model folder\n",
    "\n",
    "# Create a folder named _models in the current directory\n",
    "if not os.path.exists('_models'):\n",
    "    os.makedirs('_models')\n",
    "    \n",
    "model.save(f'_models/config3_rnn_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
